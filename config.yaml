# RAG-ing Modular PoC Configuration
# Updated with multi-source support and enhanced capabilities

# Module 1: Corpus & Embedding Lifecycle Configuration
data_source:
  # NEW: Multi-source configuration - process all enabled sources simultaneously
  sources:
    - type: "local_file"
      enabled: true
      path: "./data/"
      file_types: [".txt", ".md", ".pdf", ".docx", ".html"]
      description: "Local documents and research papers"
      
    - type: "confluence"
      enabled: false  # Enable when credentials are configured
      confluence:
        base_url: "${CONFLUENCE_BASE_URL}"
        username: "${CONFLUENCE_USERNAME}"
        auth_token: "${CONFLUENCE_API_TOKEN}"
        space_keys: ["FHIR", "DOCS", "RESEARCH"]
        page_filter: ["Implementation", "Guide", "FHIR", "Clinical"]
        max_pages: 100
      description: "Company documentation from Confluence"
      
    - type: "jira"
      enabled: false  # Enable when credentials are configured  
      jira:
        server_url: "${JIRA_URL}"
        username: "${JIRA_USER}"
        auth_token: "${JIRA_TOKEN}"
        project_keys: ["ONCOLOGY", "RESEARCH", "CLINICAL"]
        issue_types: ["Story", "Task", "Bug", "Epic", "Requirement"]
        jql_filter: "project in (ONCOLOGY, RESEARCH) AND status != Closed"
        max_issues: 200
      description: "Project tickets and requirements from Jira"

chunking:
  strategy: "recursive"  # recursive | semantic
  chunk_size: 1200  # Increased from 512 for better context preservation
  overlap: 100       # Increased from 64 for better continuity
  
  # Metadata preservation for enhanced retrieval
  prepend_metadata: true
  chunk_size_includes_metadata: false

embedding_model:
  # PRIMARY: Azure OpenAI embedding model (higher quality, no SSL issues)
  provider: "azure_openai"  # azure_openai | huggingface
  
  # Azure OpenAI embedding configuration  
  azure_model: "text-embedding-ada-002"  # or text-embedding-3-large, text-embedding-3-small
  azure_endpoint: "${AZURE_OPENAI_EMBEDDING_ENDPOINT}"
  azure_api_key: "${AZURE_OPENAI_EMBEDDING_API_KEY}"
  azure_deployment_name: "text-embedding-ada-002"  # Your actual deployment name
  azure_api_version: "${AZURE_OPENAI_EMBEDDING_API_VERSION}"
  # FALLBACK: Open source model (for development/offline use)
  fallback_model: "all-MiniLM-L6-v2"  # Standard sentence transformer model
  device: "cpu"
  
  # Model selection strategy
  use_azure_primary: true  # âœ… ENABLED - Using Azure embeddings now!

# Module 2: Enhanced Query Processing & Retrieval Configuration  
retrieval:
  # Basic retrieval settings
  top_k: 10                     # Initial retrieval count
  strategy: "hybrid"            # hybrid: semantic + BM25 keyword search
  
  # Hybrid search configuration
  semantic_weight: 0.6          # Weight for semantic similarity search
  keyword_weight: 0.4           # Weight for BM25 keyword search
  
  # Reranking with cross-encoder for relevance improvement
  reranking:
    enabled: true
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_k_initial: 20           # Retrieve more docs for reranking
    top_k_final: 5              # Final number after reranking
    relevance_threshold: 0.7    # Minimum relevance score
    
  # Context optimization
  max_context_tokens: 12000     # Increased from GraphRAG defaults
  context_precision_threshold: 0.7
  
  # Enhanced question-answer matching
  query_enhancement:
    question_keywords_boost: 2.0    # Boost chunks with question keywords
    date_keywords_boost: 1.8        # Boost temporal information
    direct_answer_boost: 2.5        # Boost direct answer patterns
    
  # Medical domain optimizations  
  domain_specific:
    medical_terms_boost: true
    ontology_codes_weight: 1.5      # Boost ICD-O, SNOMED-CT, MeSH codes
    
  # Filtering options
  filters:
    ontology_match: true
    date_range: "last_12_months"

# Module 3: Enhanced LLM Orchestration Configuration
# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# SUPPORTED PROVIDERS (OpenAI and Anthropic removed for PoC simplicity):
#   - azure_openai: Production use, Azure OpenAI service (gpt-4, gpt-5-nano, etc.)
#   - koboldcpp: Local development, self-hosted models via KoboldCpp API
# =============================================================================
llm:
  model: "gpt-5-nano"  # Your actual deployed model
  provider: "azure_openai"  # Options: azure_openai | koboldcpp

  # Standard settings for your deployed model
  max_tokens: 4096  # Reduced from 16384 
  temperature: 0.1   # Lower for more focused responses
  
  # Token management
  use_smart_truncation: true
  context_optimization: true
  token_buffer: 2000  # Reduced from 6000
  
  # Azure OpenAI configuration (cloud - primary)
  azure_endpoint: "${AZURE_OPENAI_ENDPOINT}"
  azure_api_key: "${AZURE_OPENAI_API_KEY}"
  azure_api_version: "${AZURE_OPENAI_API_VERSION}"  # Standard API version
  azure_deployment_name: "${AZURE_DEPLOYMENT_NAME}"  # Your actual deployment
  
  # Local fallback configuration (koboldcpp)
  api_url: "http://localhost:5000/v1"  # For local models
  prompt_template: "./prompts/oncology.txt"
  system_instruction: "You are an AI assistant that provides clear, concise answers. IMPORTANT: Give direct answers first, then supporting details. For questions about when something started, provide the specific date or timeframe immediately. Format responses with clear structure and avoid lengthy technical paragraphs."
  
  # Response formatting preferences
  response_style: "concise"           # concise | detailed | technical
  max_response_length: 300           # Target response length in words
  prioritize_direct_answers: true    # Put direct answers first
  include_source_references: true    # Show source documents

# Module 4: UI Layer Configuration
ui:
  framework: "fastapi"  # Current framework (streamlit archived)
  port: 8000
  host: "0.0.0.0"
  debug: false
  
  # UI Features
  show_source_metadata: true
  enable_search_history: true
  response_streaming: false

# Module 5: Enhanced Evaluation & Logging Configuration
evaluation:
  # RAGAS evaluation framework
  ragas:
    enabled: true
    # Core RAGAS metrics
    context_precision: true      # How many relevant documents were retrieved
    context_recall: true         # How many relevant documents from ground truth were retrieved  
    faithfulness: true           # How factually consistent response is with context
    answer_relevancy: true       # How relevant answer is to the query
    answer_similarity: true      # Semantic similarity to reference answers
    answer_correctness: true     # Factual accuracy compared to ground truth
    
    # Quality thresholds
    thresholds:
      context_precision: 0.75
      context_recall: 0.70
      faithfulness: 0.85
      answer_relevancy: 0.80
      answer_similarity: 0.80
      answer_correctness: 0.70
  
  # Continuous evaluation framework
  continuous:
    enabled: true
    batch_size: 10
    evaluation_interval: 100
    sample_rate: 0.1           # Evaluate 10% of queries in real-time
    alert_on_degradation: true
    
  # Logging configuration
  logging:
    enabled: true
    format: "json"
    path: "./logs/"
    include_ragas_scores: true
    
  # Traditional metrics (backward compatibility)
  metrics:
    - "response_time"
    - "token_count" 
    - "cost_estimate"
    - "retrieval_accuracy"
    - "context_relevance"
    - "safety_score"
    
  # Quality gates
  quality_gates:
    min_safety_score: 0.90
    max_response_time: 5.0
    min_context_relevance: 0.75
    
  # Logging configuration
  log_level: "INFO"
  export_interval: 24
  retention_days: 30

# Vector Store Configuration
# Options: "chroma" (default, <50K docs) | "faiss" (production, >50K docs, <10ms queries)
# FAISS available but not currently used - see corpus_embedding.py for switching guide
vector_store:
  type: "chroma"
  path: "./vector_store"
  collection_name: "oncology_docs"

# Duplicate Detection Configuration
# Prevents same document from being indexed multiple times
duplicate_detection:
  enabled: true
  
  # Exact match: SHA256 hash comparison (fast, catches identical docs)
  exact_match:
    enabled: true
    hash_algorithm: "sha256"
  
  # Fuzzy match: String similarity (catches minor variations)
  fuzzy_match:
    enabled: true
    similarity_threshold: 0.95  # 95% similar = duplicate
  
  # Semantic match: Embedding similarity (catches paraphrased content)
  semantic_match:
    enabled: false  # Not yet implemented
    embedding_similarity_threshold: 0.98
  
  # Storage location for hash database
  storage:
    database_path: "./vector_store/document_hashes.db"

# Activity Logging Configuration  
# Tracks user interactions for analytics and fine-tuning
activity_logging:
  enabled: true
  log_dir: "./logs/user_activity"
  log_queries: true  # Log all search queries
  log_results: true  # Log search results
  log_feedback: true  # Log user feedback (thumbs up/down)
  retention_days: 90  # Keep logs for 90 days

# Hierarchical Storage Configuration
# Dual-layer storage: summaries for broad search, chunks for details
hierarchical_storage:
  enabled: true
  use_summaries: true  # Use LLM to create document summaries
  summary_collection: "oncology_docs_summaries"  # Separate collection for summaries
  chunk_collection: "oncology_docs_chunks"  # Collection for detailed chunks
  summary_prompt: "Summarize this document in 2-3 sentences, preserving key medical terms and concepts:"
  routing_threshold: 0.7  # If summary match > 0.7, fetch detailed chunks

# Temporary Files Configuration
temp_files:
  directory: "./temp_helper_codes"
  auto_cleanup: false  # Set to true if you want automatic cleanup
  file_types: ["*.py", "*.md", "*.txt", "*.html", "*.json", "*.yaml", "*.log"]