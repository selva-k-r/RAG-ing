# RAG-ing Modular PoC Configuration
# Updated with multi-source support and enhanced capabilities

# Module 1: Corpus & Embedding Lifecycle Configuration
data_source:
  # NEW: Multi-source configuration - process all enabled sources simultaneously
  sources:
    - type: "local_file"
      enabled: false  # Disabled - PDFs cause verbose logging
      path: "./data/"
      file_types: [".txt", ".md", ".pdf", ".docx", ".html"]
      description: "Local documents and research papers"
      
    - type: "confluence"
      enabled: false  # Enable when credentials are configured
      confluence:
        base_url: "${CONFLUENCE_BASE_URL}"
        username: "${CONFLUENCE_USERNAME}"
        auth_token: "${CONFLUENCE_API_TOKEN}"
        space_keys: ["FHIR", "DOCS", "RESEARCH"]
        page_filter: ["Implementation", "Guide", "FHIR", "Clinical"]
        max_pages: 100
      description: "Company documentation from Confluence"
      
    - type: "jira"
      enabled: false  # Enable when credentials are configured  
      jira:
        server_url: "${JIRA_URL}"
        username: "${JIRA_USER}"
        auth_token: "${JIRA_TOKEN}"
        project_keys: ["PROJECT1", "PROJECT2", "PROJECT3"]
        issue_types: ["Story", "Task", "Bug", "Epic", "Requirement"]
        jql_filter: "project in (PROJECT1, PROJECT2) AND status != Closed"
        max_issues: 200
      description: "Project tickets and requirements from Jira"
      
    - type: "azure_devops"
      enabled: true
      azure_devops:
        organization: "${AZURE_DEVOPS_ORG}"
        project: "${AZURE_DEVOPS_PROJECT}"
        pat_token: "${AZURE_DEVOPS_PAT}"
        repo_name: "${AZURE_DEVOPS_REPO}"  # dbt-pophealth
        branch: "spike/rag_search"  # Default branch
        
        # PATH FILTERING - Load from manifest.json (contains all models, tests, macros)
        # Manifest.json is the single source of truth with all compiled SQL code
        include_paths:
          # ========================================
          # DBT PROJECT FILE - Required for DBT detection
          # ========================================
          - "/dbt_anthem/dbt_project.yml"       # DBT project configuration (REQUIRED)
          
          # ========================================
          # DBT MANIFEST - Complete project metadata + SQL code
          # ========================================
          - "/dbt_anthem/target/manifest.json"  # All models, tests, macros with SQL code
          
          # ========================================
          # DBT MACROS - Source files for Jinja macros (optional)
          # ========================================
          - "/dbt_anthem/macros/"        # Jinja macro source files
          
          # ========================================
          # DBT SEEDS - Reference data (optional)
          # ========================================
          - "/dbt_anthem/data/"          # CSV seed files
        
        exclude_paths:
          - "/dbt_anthem/tests/fixtures"             # Skip test fixture data
          - "/dbt_anthem/target/compiled"            # Skip compiled SQL (already in manifest)
          - "/dbt_anthem/target/run"                 # Skip run artifacts
          - "/dbt_anthem/target/partial_parse.msgpack"  # Skip parser cache
          - "/dbt_anthem/target/catalog.json"        # Skip catalog - testing manifest only
          - "/dbt_anthem/target/graph_summary.json"  # Skip graph_summary - testing manifest only
          - "/dbt_anthem/target/semantic_manifest.json"  # Skip semantic_manifest

        
        # FILE TYPE FILTERING
        include_file_types:
          - ".json"    # DBT manifest (primary source)
          - ".sql"     # Macro SQL if needed
          - ".csv"     # Seed data files
        
        exclude_file_types:
          - ".pbix"       # Power BI binaries
          - ".dll"        # Binary files
          - ".sln"        # Visual Studio files
          - ".smproj"     # SQL Server projects
          - ".gitignore"  # Git files
          - ".gitkeep"    # Git files
          - ".exe"        # Executables
          - ".bin"        # Binary files
          - ".env"        # Environment files
        
        # COMMIT HISTORY TRACKING (NEW!)
        fetch_commit_history: true    # Enable commit history
        commits_per_file: 10          # Fetch last 10 commits per file
        
        # BATCH PROCESSING (NEW!)
        enable_streaming: true        # Process files in batches (recommended)
        batch_size: 50                # Files per batch (50 = ~3-5 min per batch)
        
      description: "Project files and documentation from Azure DevOps with commit history"

chunking:
  strategy: "recursive"  # recursive | semantic
  chunk_size: 3000  # Increased to 3000 for better context
  overlap: 200       # Increased overlap proportionally
  max_chunks: null    # No limit - process all chunks for production
  
  # Metadata preservation for enhanced retrieval
  prepend_metadata: true
  chunk_size_includes_metadata: false

# ============================================================================
# EMBEDDING MODEL - Azure OpenAI Only
# ============================================================================
embedding_model:
  azure_openai:
    model: "text-embedding-ada-002"
    endpoint: "${AZURE_OPENAI_EMBEDDING_ENDPOINT}"
    api_key: "${AZURE_OPENAI_EMBEDDING_API_KEY}"
    deployment_name: "text-embedding-ada-002"
    api_version: "${AZURE_OPENAI_EMBEDDING_API_VERSION}"
    # Rate limiting to avoid "too many requests" errors
    max_retries: 5
    retry_delay: 2  # seconds
    requests_per_minute: 6000  # Updated: 6000 requests/min with 1M token limit

# Module 2: Enhanced Query Processing & Retrieval Configuration  
retrieval:
  # Basic retrieval settings
  top_k: 15                     # Increased to capture stg_qm1 (was at position 14)
  strategy: "hybrid"            # hybrid: semantic + BM25 keyword search
  
  # NEW: Multi-query expansion settings
  query_expansion:
    enabled: true                           # Enable multi-query expansion
    num_variations: 9                       # Generate 9 alternative questions
    use_project_detection: true             # Detect project from query
    expansion_prompt: "./prompts/query_expansion.txt"
    llm_temperature: 0.3                    # Low temperature for consistent variations
    cache_expansions: true                  # Cache expansions for similar queries
    cache_ttl: 3600                         # Cache for 1 hour (seconds)
  
  # NEW: Multi-query retrieval settings
  multi_query:
    enabled: true                           # Enable multi-query retrieval
    k_per_query: 10                         # Fetch top 10 chunks per query variation
    aggregation_method: "frequency_relevance"  # Options: frequency_relevance | max_score | avg_score
    parallel_execution: true                # Fetch embeddings/queries in parallel
    min_frequency_threshold: 2              # Chunk must appear in at least 2 queries
  
  # NEW: Hybrid context assembly (semantic + keyword split)
  hybrid_context:
    semantic_weight: 0.7                    # 70% from semantic multi-query results
    keyword_weight: 0.3                     # 30% from keyword BM25 results
    total_chunks: 10                        # Total chunks to pass to LLM
    deduplication: true                     # Remove duplicate chunks
  
  # Hybrid search configuration (existing - used for keyword component)
  semantic_weight: 0.5          # Reduced semantic weight slightly
  keyword_weight: 0.5           # Increased keyword weight for better model name matching
  
  # Reranking with cross-encoder for relevance improvement
  reranking:
    enabled: true
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_k_initial: 30           # Increased to ensure stg_qm1 is included
    top_k_final: 5              # Reduced to prevent context overflow (was 10)
    relevance_threshold: 0.5    # Lowered threshold to be more inclusive
    
  # Context optimization
  max_context_tokens: 12000     # Increased from GraphRAG defaults
  context_precision_threshold: 0.7
  
  # Enhanced question-answer matching
  query_enhancement:
    question_keywords_boost: 2.0    # Boost chunks with question keywords
    date_keywords_boost: 1.8        # Boost temporal information
    direct_answer_boost: 2.5        # Boost direct answer patterns
    
  # Medical domain optimizations  
  domain_specific:
    medical_terms_boost: true
    ontology_codes_weight: 1.5      # Boost ICD-O, SNOMED-CT, MeSH codes
    
  # Filtering options
  filters:
    ontology_match: true
    date_range: "last_12_months"

# Module 3: Enhanced LLM Orchestration Configuration
# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# SUPPORTED PROVIDERS (OpenAI and Anthropic removed for PoC simplicity):
#   - azure_openai: Production use, Azure OpenAI service (gpt-4, gpt-5-nano, etc.)
#   - koboldcpp: Local development, self-hosted models via KoboldCpp API
# =============================================================================
llm:
  model: "gpt-5-nano"  # Azure OpenAI model
  provider: "azure_openai"  # Options: azure_openai | koboldcpp

  # Standard settings for your deployed model
  max_tokens: 10000  # Increased from 4096 for comprehensive answers with rich formatting
  temperature: 0.2   # Lower for more focused responses
  
  # Token management
  use_smart_truncation: true
  context_optimization: true
  token_buffer: 500  # Increased available context (was 2000, too aggressive)
  
  # Azure OpenAI configuration (cloud - primary)
  azure_endpoint: "${AZURE_OPENAI_ENDPOINT}"
  azure_api_key: "${AZURE_OPENAI_API_KEY}"
  azure_api_version: "${AZURE_OPENAI_API_VERSION}"  # Standard API version
  azure_deployment_name: "${AZURE_DEPLOYMENT_NAME}"  # Your actual deployment
  
  # Local fallback configuration (koboldcpp)
  api_url: "http://localhost:5000/v1"  # For local models
  prompt_template: "./prompts/general.txt"
  
  # NEW: Enhanced answer formatting prompt
  answer_formatting_prompt: "./prompts/enhanced_answer_formatting.txt"
  use_json_mode: true  # Enable for structured output (query expansion)
  
  system_instruction: "/You are an AI assistant that provides clear, 
                      concise answers based STRICTLY on the provided context. 
                      CRITICAL: Answer ONLY from the given context - never use external knowledge. 
                      If the answer is not in the context, clearly state this and suggest rephrasing 
                      based on available information. Give direct answers first, 
                      then supporting details. Format responses with clear structure. Avoid giving out codes unless asked explicitly./"
  
  # Response formatting preferences
  response_style: "detailed"          # Changed from "concise" to support rich formatting
  max_response_length: 1000          # Increased from 300 words for comprehensive answers
  prioritize_direct_answers: true    # Put direct answers first
  include_source_references: true    # Show source documents
  rich_formatting: true              # NEW: Enable rich markdown formatting
  show_code_by_default: false        # NEW: Hide code unless explicitly requested

# Module 4: UI Layer Configuration
ui:
  framework: "fastapi"  # FastAPI web interface
  port: 8000
  host: "0.0.0.0"
  debug: false
  
  # UI Features
  show_source_metadata: true
  enable_search_history: true
  response_streaming: false

# Module 5: Enhanced Evaluation & Logging Configuration
evaluation:
  # RAGAS evaluation framework
  ragas:
    enabled: true
    # Core RAGAS metrics
    context_precision: true      # How many relevant documents were retrieved
    context_recall: true         # How many relevant documents from ground truth were retrieved  
    faithfulness: true           # How factually consistent response is with context
    answer_relevancy: true       # How relevant answer is to the query
    answer_similarity: true      # Semantic similarity to reference answers
    answer_correctness: true     # Factual accuracy compared to ground truth
    
    # Quality thresholds
    thresholds:
      context_precision: 0.75
      context_recall: 0.70
      faithfulness: 0.85
      answer_relevancy: 0.80
      answer_similarity: 0.80
      answer_correctness: 0.70
  
  # Continuous evaluation framework
  continuous:
    enabled: true
    batch_size: 10
    evaluation_interval: 100
    sample_rate: 0.1           # Evaluate 10% of queries in real-time
    alert_on_degradation: true
    
  # Logging configuration
  logging:
    enabled: true
    format: "json"
    path: "./logs/"
    include_ragas_scores: true
    
  # Traditional metrics (backward compatibility)
  metrics:
    - "response_time"
    - "token_count" 
    - "cost_estimate"
    - "retrieval_accuracy"
    - "context_relevance"
    - "safety_score"
    
  # Quality gates
  quality_gates:
    min_safety_score: 0.90
    max_response_time: 5.0
    min_context_relevance: 0.75
    
  # Logging configuration
  log_level: "INFO"
  export_interval: 24
  retention_days: 30

# Vector Store Configuration
# Options: "chroma" (default, <50K docs) | "faiss" (production, >50K docs, <10ms queries)
# FAISS available but not currently used - see corpus_embedding.py for switching guide
vector_store:
  type: "chroma"
  path: "./vector_store"
  collection_name: "rag_documents"

# Duplicate Detection Configuration
# Prevents same document from being indexed multiple times
duplicate_detection:
  enabled: true
  
  # Exact match: SHA256 hash comparison (fast, catches identical docs)
  exact_match:
    enabled: true
    hash_algorithm: "sha256"
  
  # Fuzzy match: String similarity (catches minor variations)
  fuzzy_match:
    enabled: true
    similarity_threshold: 0.95  # 95% similar = duplicate
  
  # Semantic match: Embedding similarity (catches paraphrased content)
  semantic_match:
    enabled: false  # Not yet implemented
    embedding_similarity_threshold: 0.98
  
  # Storage location for hash database
  storage:
    database_path: "./vector_store/document_hashes.db"

# Activity Logging Configuration  
# Tracks user interactions for analytics and fine-tuning
activity_logging:
  enabled: true
  log_dir: "./logs/user_activity"
  log_queries: true  # Log all search queries
  log_results: true  # Log search results
  log_feedback: true  # Log user feedback (thumbs up/down)
  retention_days: 90  # Keep logs for 90 days

# Hierarchical Storage Configuration
# Dual-layer storage: summaries for broad search, chunks for details
hierarchical_storage:
  enabled: false  # Temporarily disabled for debugging
  use_summaries: true  # Use LLM to create document summaries
  summary_collection: "rag_documents_summaries"  # Separate collection for summaries
  chunk_collection: "rag_documents_chunks"  # Collection for detailed chunks
  summary_prompt: "Summarize this document in 2-3 sentences, preserving key terms and concepts:"
  routing_threshold: 0.7  # If summary match > 0.7, fetch detailed chunks

# Temporary Files Configuration
temp_files:
  directory: "./temp_helper_codes"
  auto_cleanup: false  # Set to true if you want automatic cleanup
  file_types: ["*.py", "*.md", "*.txt", "*.html", "*.json", "*.yaml", "*.log"]