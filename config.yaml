# RAG-ing Modular PoC Configuration
# Aligned with Requirement.md specification

# Module 1: Corpus & Embedding Lifecycle Configuration
data_source:
  type: "local_file"  # Switch back to local to test with HL7 content we obtained
  path: "./data/"
  confluence:
    base_url: "${CONFLUENCE_BASE_URL}"
    username: "${CONFLUENCE_USERNAME}"
    auth_token: "${CONFLUENCE_API_TOKEN}"
    space_key: "FHIR"  # Common HL7 FHIR space
    page_filter: ["Implementation", "Guide", "FHIR"]  # Filter for relevant pages

chunking:
  strategy: "recursive"  # recursive | semantic
  chunk_size: 512
  overlap: 64

embedding_model:
  name: "pubmedbert"
  device: "cpu"

# Module 2: Query Processing & Retrieval Configuration  
retrieval:
  top_k: 5
  strategy: "similarity"  # similarity | hybrid
  filters:
    ontology_match: true
    date_range: "last_12_months"

# Module 3: LLM Orchestration Configuration
llm:
  model: "gpt-5-nano"  # Azure OpenAI deployment name
  provider: "azure_openai"  # Back to Azure OpenAI
  api_url: "http://localhost:5000/v1"  # Only used for koboldcpp
  prompt_template: "./prompts/iconnect_enterprise.txt"
  system_instruction: "You are iConnect, an AI-powered enterprise search assistant specializing in PopHealth, DataSage, DevOps, and clinical documentation."
  temperature: 1.0  # gpt-5-nano requires default temperature
  max_tokens: 800  # Increased for detailed enterprise responses

# Module 4: UI Layer Configuration
ui:
  framework: "streamlit"
  audience_toggle: true
  feedback_enabled: true
  show_chunk_metadata: true
  default_model: "biomistral"
  default_source: "confluence"

# Module 5: Evaluation & Logging Configuration
evaluation:
  metrics:
    precision_at_k: true
    citation_coverage: true
    clarity_rating: true
    latency: true
    safety: true
  logging:
    enabled: true
    format: "json"
    path: "./logs/"

# Vector Store Configuration
vector_store:
  type: "chroma"
  path: "./vector_store"
  collection_name: "oncology_docs"