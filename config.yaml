# RAG-ing Modular PoC Configuration
# Updated with multi-source support and enhanced capabilities

# Module 1: Corpus & Embedding Lifecycle Configuration
data_source:
  # NEW: Multi-source configuration - process all enabled sources simultaneously
  sources:
    - type: "local_file"
      enabled: true
      path: "./data/"
      file_types: [".txt", ".md", ".pdf", ".docx", ".html"]
      description: "Local documents and research papers"
      
    - type: "confluence"
      enabled: false  # Enable when credentials are configured
      confluence:
        base_url: "${CONFLUENCE_BASE_URL}"
        username: "${CONFLUENCE_USERNAME}"
        auth_token: "${CONFLUENCE_API_TOKEN}"
        space_keys: ["FHIR", "DOCS", "RESEARCH"]
        page_filter: ["Implementation", "Guide", "FHIR", "Clinical"]
        max_pages: 100
      description: "Company documentation from Confluence"
      
    - type: "jira"
      enabled: false  # Enable when credentials are configured  
      jira:
        server_url: "${JIRA_URL}"
        username: "${JIRA_USER}"
        auth_token: "${JIRA_TOKEN}"
        project_keys: ["PROJECT1", "PROJECT2", "PROJECT3"]
        issue_types: ["Story", "Task", "Bug", "Epic", "Requirement"]
        jql_filter: "project in (PROJECT1, PROJECT2) AND status != Closed"
        max_issues: 200
      description: "Project tickets and requirements from Jira"
      
    - type: "azure_devops"
      enabled: true
      azure_devops:
        organization: "${AZURE_DEVOPS_ORG}"
        project: "${AZURE_DEVOPS_PROJECT}"
        pat_token: "${AZURE_DEVOPS_PAT}"
        repo_name: "${AZURE_DEVOPS_REPO}"  # dbt-pophealth
        branch: "spike/rag_search"  # Default branch
        
        # PATH FILTERING - Load only specific folders
        # NOTE: Azure DevOps API requires DIRECTORY paths (not individual files)
        # Paths use startswith() matching - all files under path are fetched
        include_paths:
          # ========================================
          # DBT ARTIFACTS (REQUIRED)
          # Contains ALL SQL code (models, tests, macros) + metadata
          # Fetches: manifest.json, catalog.json, graph_summary.json
          # ========================================
          - "/dbt_anthem/target/"                    # Artifacts directory
          
          # ========================================
          # DBT PROJECT CONFIG (REQUIRED)
          # Contains project name, version, profiles
          # Fetches: dbt_project.yml
          # ========================================
          - "/dbt_anthem/dbt_project.yml"            # Project config file
          
          # ========================================
          # DBT SEED FILES (REQUIRED for business data queries)
          # CSV reference data used by models
          # ========================================
          - "/dbt_anthem/data/"                      # Seed CSV files directory
          
          # ========================================
          # NON-DBT CONTENT (optional)
          # ========================================
          # - "/Analytics 2.0/TabularModels/"       # Power BI models (if needed)
        
        exclude_paths:
          - "/dbt_anthem/tests/fixtures"             # Skip test fixture data
          - "/dbt_anthem/target/compiled"            # Skip compiled SQL (already in manifest)
          - "/dbt_anthem/target/run"                 # Skip run artifacts
          - "/dbt_anthem/target/partial_parse.msgpack"  # Skip parser cache

        
        # FILE TYPE FILTERING
        include_file_types:
          - ".json"    # DBT artifacts (manifest, catalog, graph_summary)
          - ".yml"     # DBT project config
          - ".yaml"    # Alternative YAML extension
          - ".csv"     # Seed reference data files
          # - ".sql"   # Not needed - SQL code is in manifest.json
          # - ".md"    # Not needed unless separate docs wanted
          # - ".py"    # Not needed unless Python models exist
        
        exclude_file_types:
          - ".pbix"       # Power BI binaries
          - ".dll"        # Binary files
          - ".sln"        # Visual Studio files
          - ".smproj"     # SQL Server projects
          - ".gitignore"  # Git files
          - ".gitkeep"    # Git files
          - ".exe"        # Executables
          - ".bin"        # Binary files
          - ".env"        # Environment files
        
        # COMMIT HISTORY TRACKING (NEW!)
        fetch_commit_history: true    # Enable commit history
        commits_per_file: 10          # Fetch last 10 commits per file
        
        # BATCH PROCESSING (NEW!)
        enable_streaming: true        # Process files in batches (recommended)
        batch_size: 50                # Files per batch (50 = ~3-5 min per batch)
        
      description: "Project files and documentation from Azure DevOps with commit history"

chunking:
  strategy: "recursive"  # recursive | semantic
  chunk_size: 1200  # Increased from 512 for better context preservation
  overlap: 100       # Increased from 64 for better continuity
  
  # Metadata preservation for enhanced retrieval
  prepend_metadata: true
  chunk_size_includes_metadata: false

embedding_model:
  # PRIMARY: Azure OpenAI embedding model (higher quality, no SSL issues)
  provider: "azure_openai"  # azure_openai | huggingface
  
  # Azure OpenAI embedding configuration  
  azure_model: "text-embedding-ada-002"  # or text-embedding-3-large, text-embedding-3-small
  azure_endpoint: "${AZURE_OPENAI_EMBEDDING_ENDPOINT}"
  azure_api_key: "${AZURE_OPENAI_EMBEDDING_API_KEY}"
  azure_deployment_name: "text-embedding-ada-002"  # Your actual deployment name
  azure_api_version: "${AZURE_OPENAI_EMBEDDING_API_VERSION}"
  # FALLBACK: Open source model (for development/offline use)
  fallback_model: "all-MiniLM-L6-v2"  # Standard sentence transformer model
  device: "cpu"
  
  # Model selection strategy
  use_azure_primary: true  # âœ… ENABLED - Using Azure embeddings now!

# Module 2: Enhanced Query Processing & Retrieval Configuration  
retrieval:
  # Basic retrieval settings
  top_k: 10                     # Initial retrieval count
  strategy: "hybrid"            # hybrid: semantic + BM25 keyword search
  
  # Hybrid search configuration
  semantic_weight: 0.6          # Weight for semantic similarity search
  keyword_weight: 0.4           # Weight for BM25 keyword search
  
  # Reranking with cross-encoder for relevance improvement
  reranking:
    enabled: true
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_k_initial: 20           # Retrieve more docs for reranking
    top_k_final: 5              # Final number after reranking
    relevance_threshold: 0.7    # Minimum relevance score
    
  # Context optimization
  max_context_tokens: 12000     # Increased from GraphRAG defaults
  context_precision_threshold: 0.7
  
  # Enhanced question-answer matching
  query_enhancement:
    question_keywords_boost: 2.0    # Boost chunks with question keywords
    date_keywords_boost: 1.8        # Boost temporal information
    direct_answer_boost: 2.5        # Boost direct answer patterns
    
  # Medical domain optimizations  
  domain_specific:
    medical_terms_boost: true
    ontology_codes_weight: 1.5      # Boost ICD-O, SNOMED-CT, MeSH codes
    
  # Filtering options
  filters:
    ontology_match: true
    date_range: "last_12_months"

# Module 3: Enhanced LLM Orchestration Configuration
# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# SUPPORTED PROVIDERS (OpenAI and Anthropic removed for PoC simplicity):
#   - azure_openai: Production use, Azure OpenAI service (gpt-4, gpt-5-nano, etc.)
#   - koboldcpp: Local development, self-hosted models via KoboldCpp API
# =============================================================================
llm:
  model: "gpt-5-nano"  # Azure OpenAI model
  provider: "azure_openai"  # Options: azure_openai | koboldcpp

  # Standard settings for your deployed model
  max_tokens: 4096  # Reduced from 16384 
  temperature: 0.2   # Lower for more focused responses
  
  # Token management
  use_smart_truncation: true
  context_optimization: true
  token_buffer: 500  # Increased available context (was 2000, too aggressive)
  
  # Azure OpenAI configuration (cloud - primary)
  azure_endpoint: "${AZURE_OPENAI_ENDPOINT}"
  azure_api_key: "${AZURE_OPENAI_API_KEY}"
  azure_api_version: "${AZURE_OPENAI_API_VERSION}"  # Standard API version
  azure_deployment_name: "${AZURE_DEPLOYMENT_NAME}"  # Your actual deployment
  
  # Local fallback configuration (koboldcpp)
  api_url: "http://localhost:5000/v1"  # For local models
  prompt_template: "./prompts/general.txt"
  system_instruction: "You are an AI assistant that provides clear, concise answers based STRICTLY on the provided context. CRITICAL: Answer ONLY from the given context - never use external knowledge. If the answer is not in the context, clearly state this and suggest rephrasing based on available information. Give direct answers first, then supporting details. Format responses with clear structure."
  
  # Response formatting preferences
  response_style: "concise"           # concise | detailed | technical
  max_response_length: 300           # Target response length in words
  prioritize_direct_answers: true    # Put direct answers first
  include_source_references: true    # Show source documents

# Module 4: UI Layer Configuration
ui:
  framework: "fastapi"  # FastAPI web interface
  port: 8000
  host: "0.0.0.0"
  debug: false
  
  # UI Features
  show_source_metadata: true
  enable_search_history: true
  response_streaming: false

# Module 5: Enhanced Evaluation & Logging Configuration
evaluation:
  # RAGAS evaluation framework
  ragas:
    enabled: true
    # Core RAGAS metrics
    context_precision: true      # How many relevant documents were retrieved
    context_recall: true         # How many relevant documents from ground truth were retrieved  
    faithfulness: true           # How factually consistent response is with context
    answer_relevancy: true       # How relevant answer is to the query
    answer_similarity: true      # Semantic similarity to reference answers
    answer_correctness: true     # Factual accuracy compared to ground truth
    
    # Quality thresholds
    thresholds:
      context_precision: 0.75
      context_recall: 0.70
      faithfulness: 0.85
      answer_relevancy: 0.80
      answer_similarity: 0.80
      answer_correctness: 0.70
  
  # Continuous evaluation framework
  continuous:
    enabled: true
    batch_size: 10
    evaluation_interval: 100
    sample_rate: 0.1           # Evaluate 10% of queries in real-time
    alert_on_degradation: true
    
  # Logging configuration
  logging:
    enabled: true
    format: "json"
    path: "./logs/"
    include_ragas_scores: true
    
  # Traditional metrics (backward compatibility)
  metrics:
    - "response_time"
    - "token_count" 
    - "cost_estimate"
    - "retrieval_accuracy"
    - "context_relevance"
    - "safety_score"
    
  # Quality gates
  quality_gates:
    min_safety_score: 0.90
    max_response_time: 5.0
    min_context_relevance: 0.75
    
  # Logging configuration
  log_level: "INFO"
  export_interval: 24
  retention_days: 30

# Vector Store Configuration
# Options: "chroma" (default, <50K docs) | "faiss" (production, >50K docs, <10ms queries)
# FAISS available but not currently used - see corpus_embedding.py for switching guide
vector_store:
  type: "chroma"
  path: "./vector_store"
  collection_name: "rag_documents"

# Duplicate Detection Configuration
# Prevents same document from being indexed multiple times
duplicate_detection:
  enabled: true
  
  # Exact match: SHA256 hash comparison (fast, catches identical docs)
  exact_match:
    enabled: true
    hash_algorithm: "sha256"
  
  # Fuzzy match: String similarity (catches minor variations)
  fuzzy_match:
    enabled: true
    similarity_threshold: 0.95  # 95% similar = duplicate
  
  # Semantic match: Embedding similarity (catches paraphrased content)
  semantic_match:
    enabled: false  # Not yet implemented
    embedding_similarity_threshold: 0.98
  
  # Storage location for hash database
  storage:
    database_path: "./vector_store/document_hashes.db"

# Activity Logging Configuration  
# Tracks user interactions for analytics and fine-tuning
activity_logging:
  enabled: true
  log_dir: "./logs/user_activity"
  log_queries: true  # Log all search queries
  log_results: true  # Log search results
  log_feedback: true  # Log user feedback (thumbs up/down)
  retention_days: 90  # Keep logs for 90 days

# Hierarchical Storage Configuration
# Dual-layer storage: summaries for broad search, chunks for details
hierarchical_storage:
  enabled: false  # Temporarily disabled for debugging
  use_summaries: true  # Use LLM to create document summaries
  summary_collection: "rag_documents_summaries"  # Separate collection for summaries
  chunk_collection: "rag_documents_chunks"  # Collection for detailed chunks
  summary_prompt: "Summarize this document in 2-3 sentences, preserving key terms and concepts:"
  routing_threshold: 0.7  # If summary match > 0.7, fetch detailed chunks

# Temporary Files Configuration
temp_files:
  directory: "./temp_helper_codes"
  auto_cleanup: false  # Set to true if you want automatic cleanup
  file_types: ["*.py", "*.md", "*.txt", "*.html", "*.json", "*.yaml", "*.log"]