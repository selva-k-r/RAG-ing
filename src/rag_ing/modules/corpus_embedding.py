"""Module 1: Corpus & Embedding Lifecycle\n\nObjective: Ingest oncology-related documents, generate embeddings, and store them for retrieval.\n\"\"\"\n\nimport logging\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma, FAISS\nfrom transformers import AutoTokenizer, AutoModel\nimport chromadb\nfrom ..config.settings import Settings, DataSourceConfig, ChunkingConfig, EmbeddingModelConfig\nfrom ..connectors import ConfluenceConnector\nfrom ..utils.exceptions import IngestionError\n\nlogger = logging.getLogger(__name__)\n\n\nclass CorpusEmbeddingModule:\n    \"\"\"Module for YAML-driven document ingestion and embedding generation.\"\"\"\n    \n    def __init__(self, config: Settings):\n        self.config = config\n        self.data_source_config = config.data_source\n        self.chunking_config = config.chunking\n        self.embedding_config = config.embedding_model\n        self.vector_store_config = config.vector_store\n        \n        self.embedding_model = None\n        self.vector_store = None\n        self._stats = {\n            \"chunk_count\": 0,\n            \"vector_size\": 0,\n            \"processing_time\": 0\n        }\n    \n    def process_corpus(self) -> Dict[str, Any]:\n        \"\"\"Main entry point for corpus processing.\"\"\"\n        logger.info(\"Starting corpus processing pipeline\")\n        start_time = time.time()\n        \n        try:\n            # Step 1: YAML-Driven Ingestion Logic\n            documents = self._ingest_documents()\n            logger.info(f\"Ingested {len(documents)} documents\")\n            \n            # Step 2: Chunking Strategy\n            chunks = self._chunk_documents(documents)\n            logger.info(f\"Created {len(chunks)} chunks\")\n            \n            # Step 3: Embedding Generation\n            self._load_embedding_model()\n            \n            # Step 4: Vector Storage\n            self._setup_vector_store()\n            self._store_embeddings(chunks)\n            \n            processing_time = time.time() - start_time\n            self._stats.update({\n                \"chunk_count\": len(chunks),\n                \"vector_size\": self.embedding_model.client.get_sentence_embedding_dimension() if hasattr(self.embedding_model, 'client') else 768,\n                \"processing_time\": processing_time\n            })\n            \n            logger.info(f\"Corpus processing completed in {processing_time:.2f}s\")\n            return self._stats\n            \n        except Exception as e:\n            logger.error(f\"Corpus processing failed: {e}\")\n            raise IngestionError(f\"Failed to process corpus: {e}\")\n    \n    def _ingest_documents(self) -> List[Document]:\n        \"\"\"YAML-driven document ingestion.\"\"\"\n        data_source_type = self.data_source_config.type\n        \n        if data_source_type == \"local_file\":\n            return self._ingest_local_files()\n        elif data_source_type == \"confluence\":\n            return self._ingest_confluence_docs()\n        else:\n            raise ValueError(f\"Unsupported data source type: {data_source_type}\")\n    \n    def _ingest_local_files(self) -> List[Document]:\n        \"\"\"Ingest documents from local file path.\"\"\"\n        path = Path(self.data_source_config.path)\n        documents = []\n        \n        if not path.exists():\n            raise FileNotFoundError(f\"Data path does not exist: {path}\")\n        \n        for file_path in path.rglob(\"*\"):\n            if file_path.is_file() and file_path.suffix in [\".txt\", \".md\", \".pdf\"]:\n                try:\n                    content = self._extract_file_content(file_path)\n                    doc = Document(\n                        page_content=content,\n                        metadata={\n                            \"source\": str(file_path),\n                            \"filename\": file_path.name,\n                            \"file_type\": file_path.suffix,\n                            \"date\": file_path.stat().st_mtime,\n                            \"ontology_codes\": self._extract_ontology_codes(content)\n                        }\n                    )\n                    documents.append(doc)\n                except Exception as e:\n                    logger.warning(f\"Failed to process file {file_path}: {e}\")\n        \n        return documents\n    \n    def _ingest_confluence_docs(self) -> List[Document]:\n        \"\"\"Ingest documents from Confluence.\"\"\"\n        confluence_config = self.data_source_config.confluence\n        if not confluence_config:\n            raise ValueError(\"Confluence configuration is required for confluence data source\")\n        \n        connector = ConfluenceConnector(confluence_config)\n        \n        # Authenticate and connect\n        if not connector.connect():\n            raise ConnectionError(\"Failed to connect to Confluence\")\n        \n        # Fetch documents with space key and page filters\n        documents = connector.fetch_documents(\n            space_key=confluence_config.get(\"space_key\"),\n            page_filter=confluence_config.get(\"page_filter\", [])\n        )\n        \n        # Enhance metadata with ontology codes\n        for doc in documents:\n            doc.metadata[\"ontology_codes\"] = self._extract_ontology_codes(doc.page_content)\n        \n        return documents\n    \n    def _extract_file_content(self, file_path: Path) -> str:\n        \"\"\"Extract content from different file types.\"\"\"\n        if file_path.suffix in [\".txt\", \".md\"]:\n            return file_path.read_text(encoding='utf-8')\n        elif file_path.suffix == \".pdf\":\n            # Would implement PDF extraction here\n            return f\"PDF content from {file_path.name}\"\n        else:\n            return \"\"\n    \n    def _extract_ontology_codes(self, content: str) -> List[str]:\n        \"\"\"Extract medical ontology codes from content.\"\"\"\n        ontology_codes = []\n        \n        # Simple regex patterns for common ontology codes\n        import re\n        \n        # ICD-O codes (e.g., C78.0)\n        icd_o_pattern = r'C\\d{2}\\.\\d'\n        ontology_codes.extend(re.findall(icd_o_pattern, content))\n        \n        # SNOMED-CT codes (numeric)\n        snomed_pattern = r'SNOMED[:\\s]*(\\d{6,})'  \n        ontology_codes.extend(re.findall(snomed_pattern, content))\n        \n        return list(set(ontology_codes))\n    \n    def _chunk_documents(self, documents: List[Document]) -> List[Document]:\n        \"\"\"Apply chunking strategy based on configuration.\"\"\"\n        strategy = self.chunking_config.strategy\n        \n        if strategy == \"recursive\":\n            return self._recursive_chunking(documents)\n        elif strategy == \"semantic\":\n            return self._semantic_chunking(documents)\n        else:\n            raise ValueError(f\"Unsupported chunking strategy: {strategy}\")\n    \n    def _recursive_chunking(self, documents: List[Document]) -> List[Document]:\n        \"\"\"Apply recursive character-based chunking.\"\"\"\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunking_config.chunk_size,\n            chunk_overlap=self.chunking_config.overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        \n        return text_splitter.split_documents(documents)\n    \n    def _semantic_chunking(self, documents: List[Document]) -> List[Document]:\n        \"\"\"Apply semantic boundary-aware chunking for oncology content.\"\"\"\n        chunks = []\n        \n        for doc in documents:\n            content = doc.page_content\n            semantic_boundaries = self.chunking_config.semantic_boundaries\n            \n            # Split by semantic boundaries first\n            sections = [content]\n            for boundary in semantic_boundaries:\n                new_sections = []\n                for section in sections:\n                    new_sections.extend(section.split(boundary))\n                sections = [s.strip() for s in new_sections if s.strip()]\n            \n            # Apply size-based chunking within sections\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=self.chunking_config.chunk_size,\n                chunk_overlap=self.chunking_config.overlap\n            )\n            \n            for i, section in enumerate(sections):\n                if len(section) > self.chunking_config.chunk_size:\n                    section_docs = text_splitter.split_text(section)\n                    for j, chunk_text in enumerate(section_docs):\n                        chunk = Document(\n                            page_content=chunk_text,\n                            metadata={\n                                **doc.metadata,\n                                \"chunk_id\": f\"{doc.metadata.get('source', 'unknown')}_{i}_{j}\",\n                                \"section_id\": i\n                            }\n                        )\n                        chunks.append(chunk)\n                else:\n                    chunk = Document(\n                        page_content=section,\n                        metadata={\n                            **doc.metadata,\n                            \"chunk_id\": f\"{doc.metadata.get('source', 'unknown')}_{i}\",\n                            \"section_id\": i\n                        }\n                    )\n                    chunks.append(chunk)\n        \n        return chunks\n    \n    def _load_embedding_model(self) -> None:\n        \"\"\"Load embedding model based on configuration.\"\"\"\n        model_name = self.embedding_config.name\n        device = self.embedding_config.device\n        \n        logger.info(f\"Loading embedding model: {model_name} on {device}\")\n        \n        if model_name == \"pubmedbert\":\n            model_path = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n        elif model_name == \"clinicalbert\":\n            model_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n        elif model_name == \"biobert\":\n            model_path = \"dmis-lab/biobert-v1.1\"\n        else:\n            model_path = self.embedding_config.model_path or model_name\n        \n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=model_path,\n            model_kwargs={'device': device},\n            encode_kwargs={'normalize_embeddings': True}\n        )\n        \n        logger.info(f\"Embedding model loaded successfully\")\n    \n    def _setup_vector_store(self) -> None:\n        \"\"\"Setup vector store based on configuration.\"\"\"\n        store_type = self.vector_store_config.type\n        \n        if store_type == \"chroma\":\n            self._setup_chroma_store()\n        elif store_type == \"faiss\":\n            self._setup_faiss_store()\n        else:\n            raise ValueError(f\"Unsupported vector store type: {store_type}\")\n    \n    def _setup_chroma_store(self) -> None:\n        \"\"\"Setup ChromaDB vector store.\"\"\"\n        persist_directory = self.vector_store_config.path\n        collection_name = self.vector_store_config.collection_name\n        \n        # Ensure directory exists\n        Path(persist_directory).mkdir(parents=True, exist_ok=True)\n        \n        # Initialize ChromaDB client\n        client = chromadb.PersistentClient(path=persist_directory)\n        \n        self.vector_store = Chroma(\n            client=client,\n            collection_name=collection_name,\n            embedding_function=self.embedding_model\n        )\n        \n        logger.info(f\"ChromaDB vector store initialized at {persist_directory}\")\n    \n    def _setup_faiss_store(self) -> None:\n        \"\"\"Setup FAISS vector store.\"\"\"\n        # FAISS will be initialized when first documents are added\n        self.vector_store = None\n        logger.info(\"FAISS vector store will be initialized on first document addition\")\n    \n    def _store_embeddings(self, chunks: List[Document]) -> None:\n        \"\"\"Store document chunks with embeddings in vector store.\"\"\"\n        if not chunks:\n            logger.warning(\"No chunks to store\")\n            return\n        \n        logger.info(f\"Storing {len(chunks)} chunks in vector store\")\n        \n        if self.vector_store_config.type == \"chroma\":\n            # Add documents to existing Chroma collection\n            self.vector_store.add_documents(chunks)\n        elif self.vector_store_config.type == \"faiss\":\n            # Create FAISS index from documents\n            self.vector_store = FAISS.from_documents(\n                chunks, \n                self.embedding_model\n            )\n            # Save FAISS index\n            index_path = Path(self.vector_store_config.path) / \"faiss_index\"\n            index_path.parent.mkdir(parents=True, exist_ok=True)\n            self.vector_store.save_local(str(index_path))\n        \n        logger.info(f\"Successfully stored {len(chunks)} chunks\")\n    \n    def validate_embeddings(self) -> bool:\n        \"\"\"Validate vector dimensions and schema.\"\"\"\n        if not self.vector_store:\n            return False\n        \n        try:\n            # Test embedding generation\n            test_text = \"Oncology biomarker analysis\"\n            test_embedding = self.embedding_model.embed_query(test_text)\n            \n            expected_dim = 768  # Default for most BERT models\n            actual_dim = len(test_embedding)\n            \n            if actual_dim != expected_dim:\n                logger.warning(f\"Embedding dimension mismatch: expected {expected_dim}, got {actual_dim}\")\n            \n            self._stats[\"vector_size\"] = actual_dim\n            logger.info(f\"Embedding validation successful: {actual_dim}D vectors\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Embedding validation failed: {e}\")\n            return False\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get processing statistics.\"\"\"\n        return self._stats.copy()\n    \n    def get_vector_store(self):\n        \"\"\"Get the initialized vector store.\"\"\"\n        return self.vector_store